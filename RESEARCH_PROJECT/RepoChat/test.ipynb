{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/huggingface/transformers/archive/refs/heads/master.zip\n",
      "Error converting file:  transformers-main/tests/fixtures/test_sentencepiece_bpe.model\n",
      "Error converting file:  transformers-main/tests/fixtures/spiece.model\n",
      "Error converting file:  transformers-main/tests/fixtures/test_sentencepiece_bpe_char.model\n",
      "Error converting file:  transformers-main/tests/fixtures/test_sentencepiece_no_bos.model\n",
      "Error converting file:  transformers-main/tests/fixtures/test_sentencepiece.model\n",
      "Error converting file:  transformers-main/tests/fixtures/test_sentencepiece_with_bytefallback.model\n",
      "Exiting: Cleaning up .chroma directory\n",
      "Error converting file:  transformers-main/examples/legacy/seq2seq/test_data/wmt_en_ro/val.len\n",
      "Error converting file:  transformers-main/examples/legacy/seq2seq/test_data/wmt_en_ro/train.len\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "\n",
    "# Replace with the repository URL you want to download\n",
    "repo_url = \"https://github.com/user/repo.git\"\n",
    "output_pdf = \"output.pdf\"\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "import PyPDF2\n",
    "from io import BytesIO\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate,Preformatted\n",
    "from reportlab.platypus import Image  as RLImage\n",
    "from reportlab.platypus import Paragraph, Spacer\n",
    "\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.utils import ImageReader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def download_repo_zip(link, output_folder = \"main.zip\"):\n",
    "    username =  link.split('/')[3]\n",
    "    repo = link.split('/')[4]\n",
    "    # zip_url = f\"https://github.com/{username}/{repo}/archive/refs/heads/main.zip\"\n",
    "    zip_url = f\"https://github.com/{username}/{repo}/archive/refs/heads/master.zip\"\n",
    "    print(zip_url)\n",
    "\n",
    "    response = requests.get(zip_url)\n",
    "    response.raise_for_status()\n",
    "    #down load the zip file\n",
    "    with open('main.zip', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    # return BytesIO(response.content)\n",
    "\n",
    "def extract_zip(zip_file, destination_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(destination_folder)\n",
    "    #get the name of the extracted folder\n",
    "    folder_name = zf.namelist()[0]\n",
    "    return folder_name\n",
    "\n",
    "def convert_to_pdf(input_path, output_path):\n",
    "    if input_path.endswith(\".pdf\"):\n",
    "        # Create a new PDF with the file path heading\n",
    "        buffer = BytesIO()\n",
    "        doc = SimpleDocTemplate(buffer, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        elements = []\n",
    "        heading = Paragraph(f\"File path: {input_path}\", styles[\"Heading2\"])\n",
    "        elements.append(heading)\n",
    "        elements.append(Spacer(1, 12))\n",
    "        doc.build(elements)\n",
    "\n",
    "        # Read the newly created PDF with heading\n",
    "        buffer.seek(0)\n",
    "        new_pdf = PdfFileReader(buffer)\n",
    "\n",
    "        # Read the input PDF\n",
    "        with open(input_path, \"rb\") as f:\n",
    "            input_pdf = PdfFileReader(f)\n",
    "\n",
    "        # Merge the new PDF with heading and the input PDF\n",
    "        pdf_writer = PdfFileWriter()\n",
    "        for page_num in range(new_pdf.getNumPages()):\n",
    "            pdf_writer.addPage(new_pdf.getPage(page_num))\n",
    "\n",
    "        for page_num in range(input_pdf.getNumPages()):\n",
    "            pdf_writer.addPage(input_pdf.getPage(page_num))\n",
    "\n",
    "        # Save the merged PDF to the output file\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pdf_writer.write(f)\n",
    "\n",
    "    elif input_path.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\")):\n",
    "        img = Image.open(input_path)\n",
    "        img_reader = ImageReader(img)\n",
    "        img_width, img_height = img.size\n",
    "        aspect_ratio = img_height / img_width\n",
    "\n",
    "\n",
    "        max_pdf_width = letter[0] - 2 * 72  # 1 inch margin on each side\n",
    "        max_pdf_height = letter[1] - 2 * 72  # 1 inch margin on top and bottom\n",
    "\n",
    "        if img_width > max_pdf_width:\n",
    "                img_width = max_pdf_width\n",
    "                img_height = img_width * aspect_ratio\n",
    "        if img_height > max_pdf_height:\n",
    "            img_height = max_pdf_height\n",
    "            img_width = img_height / aspect_ratio\n",
    "        img_width = int(img_width)\n",
    "        img_height = int(img_height)\n",
    "        # Resize the image\n",
    "        img = img.resize((int(img_width), int(img_height)))\n",
    "\n",
    "        img = img.resize((int(img_width), int(img_height)))\n",
    " \n",
    "        img.save(output_path, \"PNG\")\n",
    "        # Create a new PDF with the image\n",
    "        doc = SimpleDocTemplate(output_path, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "\n",
    "        elements = []\n",
    "        heading = Paragraph(f\" {input_path}\", styles[\"Heading2\"])\n",
    "        elements.append(heading)\n",
    "        elements.append(Spacer(1, 12))\n",
    "\n",
    "        img_rl = RLImage(input_path, width=img_width, height=img_height, kind='proportional')\n",
    "        elements.append(img_rl)\n",
    "\n",
    "        doc.build(elements)\n",
    "\n",
    "    else:\n",
    "        with open(input_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "   \n",
    "        doc = SimpleDocTemplate(output_path, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        elements = []\n",
    "\n",
    "        # Add the file path heading\n",
    "        heading = Paragraph(f\"{input_path}\", styles[\"Heading2\"])\n",
    "        elements.append(heading)\n",
    "        elements.append(Spacer(1, 12))\n",
    "\n",
    "        # Add the content as Preformatted text\n",
    "        text = Preformatted(content, style=styles[\"Code\"])\n",
    "        elements.append(text)\n",
    "\n",
    "        doc.build(elements)\n",
    "\n",
    "def merge_pdfs(pdf_files, output_path):\n",
    "    pdf_writer = PyPDF2.PdfWriter()\n",
    "    for pdf_file in pdf_files:\n",
    "        with open(pdf_file, \"rb\") as f:\n",
    "            try:\n",
    "                pdf_reader = PyPDF2.PdfReader(f)\n",
    "                if pdf_reader.is_encrypted:\n",
    "                    print(f\"{pdf_file} is encrypted. Skipping.\")\n",
    "                    continue\n",
    "            except:\n",
    "                print(f\"{pdf_file} is not a valid PDF. Skipping.\")\n",
    "                continue\n",
    "      \n",
    "                    \n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pdf_writer.write(f)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    download_repo_zip('https://github.com/huggingface/transformers')\n",
    "    folder_name = extract_zip('./main.zip', './')\n",
    "    ingnore_list = ['__pycache__',]\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(folder_name):\n",
    "        for file in files:\n",
    "            \n",
    "            input_file = os.path.join(root, file)\n",
    "            #if the file contains any of the strings in the ignore list, skip it\n",
    "            if any(x in input_file for x in ingnore_list):\n",
    "                continue\n",
    "            #create a temp folder to store the pdf files\n",
    "            os.makedirs(\"temp\", exist_ok=True)\n",
    "            output_file = os.path.join(\"temp\", os.path.splitext(file)[0] + \".pdf\")\n",
    "\n",
    "            try:\n",
    "                convert_to_pdf(input_file, output_file)\n",
    "            except:\n",
    "                print(\"Error converting file: \", input_file)\n",
    "                continue\n",
    "            pdf_files.append(output_file)\n",
    "\n",
    "    merged_pdf = \"merged.pdf\"\n",
    "\n",
    "    merge_pdfs(pdf_files, merged_pdf)\n",
    "    #clean up the temp folder and downloaded zip file\n",
    "    os.remove(\"main.zip\")\n",
    "    shutil.rmtree(folder_name)\n",
    "    shutil.rmtree(\"temp\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "# loader = UnstructuredPDFLoader(\"merged.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "# index = VectorstoreIndexCreator(vectorstore_cls = FAISS).from_loaders([loader])\n",
    "# index = VectorstoreIndexCreator(vectorstore_cls = FAISS)\n",
    "# index.vectorstore.load_local('asd.json')\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.load_local('asd.json',embeddings =embeddings)\n",
    "qa =VectorDBQA.from_chain_type(llm =OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"), chain_type = \"stuff\",vectorstore = vectorstore )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.vectorstore.save_local('asd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'duckdb.DuckDBPyConnection' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mindex.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     pickle\u001b[39m.\u001b[39mdump(index, f)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'duckdb.DuckDBPyConnection' object"
     ]
    }
   ],
   "source": [
    "# index.vectorstore.save_local(\"index\")\n",
    "#save the index to a local file pkl \n",
    "import pickle\n",
    "with open(\"index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This repository is for a novel approach for high-fidelity image inpainting called MISF (Multi-level Interactive Siamese Filtering). It uses a single predictive network to conduct predictive filtering at the image level and deep feature level simultaneously, leading to high-fidelity inpainting results. The method outperforms state-of-the-art methods on three public datasets. The repository includes code for training and testing the model, as well as pretrained models and datasets.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('what is this repo for ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To use this code, follow these steps:\n",
       "\n",
       "1. Download the necessary datasets (Places2, CelebA, and Dunhuang) and place them in the appropriate folders as specified in the code.\n",
       "\n",
       "2. Download the mask file and place it in the appropriate folder as specified in the code.\n",
       "\n",
       "3. Run the `data_list.py` script to generate the data list.\n",
       "\n",
       "4. Train the model using the `train.py` script and the parameters specified in the `config.yml` file.\n",
       "\n",
       "5. Test the model using the `test.py` script and the parameters specified in the `config.yml` file.\n",
       "\n",
       "Note that you will need Python 3.7 and PyTorch 1.0 or higher to run this code."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(qa.run('How can I use this. Example code ? Step by step')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To use the Experiment Tracking feature with Accelerate, you can follow these steps:\n",
       "\n",
       "1. Make sure you have the necessary experiment trackers installed, such as Weights and Biases, TensorBoard, or CometML.\n",
       "2. Add the `with_tracking` argument at the end of your command for starting the python script, such as `accelerate launch ./tracking.py --with_tracking`.\n",
       "3. In your script, use `Accelerate.init_trackers` to initialize the experiment trackers, and `Accelerator.log` to log your experiment metrics.\n",
       "4. Run your script as usual, and the experiment metrics will be automatically tracked by the experiment trackers you have installed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(qa.run('how can I use this Experiment trackers ? Step by step')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To perform gradient accumulation with Accelerate, you can use the `Accelerator.accumulate` function and specify a `gradient_accumulation_steps` parameter. This will automatically handle gradient synchronization, check if the step should be performed, and auto-scale the loss. Here is an example code snippet:\n",
       "\n",
       "```\n",
       "accelerator = Accelerator(gradient_accumulation_steps=2)\n",
       "\n",
       "model, optimizer, training_dataloader = accelerator.prepare(model, optimizer, training_dataloader)\n",
       "\n",
       "for input, label in training_dataloader:\n",
       "    with accelerator.accumulate(model):\n",
       "        predictions = model(input)\n",
       "        loss = loss_function(predictions, label)\n",
       "        accelerator.backward(loss)\n",
       "        optimizer.step()\n",
       "        scheduler.step()\n",
       "        optimizer.zero_grad()\n",
       "```\n",
       "\n",
       "Note that this is just an example and you may need to adjust the code to fit your specific use case."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# qa.run('how can I Performing gradient accumulation with Accelerate? Step by step')\n",
    "# repogpt.init_agent('https://github.com/tsingqguo/misf',api_key='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')\n",
    "https://github.com/tsingqguo/misf\n",
    "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\n",
    "#jupyter note book show markdown\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "display(Markdown(qa.run('how can I Performing gradient accumulation with Accelerate? Step by step?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('langchain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2978cd2d86bc9abc2355df98255f1b515694a55973a58299a51240d58677b487"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
