{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import create_eva_vit_g\n",
    "# model = create_eva_vit_g(precision=\"fp16\").cuda()\n",
    "# import torch\n",
    "# with torch.cuda.amp.autocast():\n",
    "#     inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "#     result = model(inputs)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from box import Box\n",
    "\n",
    "with open('/home/huimingsun/Desktop/NGP/video_sgements/config/config.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "config = Box(config_dict)\n",
    "\n",
    "# Now you can access values in the config like this:\n",
    "vit_model = config.model.vit_model\n",
    "vit_model_path = config.model.vit_model_path\n",
    "# ... and so on for the other values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "import torch.nn as nn\n",
    "from eva_vit import Trans_Block\n",
    "\n",
    "try:\n",
    "    from .blip2 import Blip2Base, disabled_train\n",
    "except:\n",
    "    from blip2 import Blip2Base, disabled_train\n",
    "\n",
    "\n",
    "class VSeg(Blip2Base):\n",
    "    \"\"\"\n",
    "    VideoChat model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "        self.low_resource = config.low_resource\n",
    "\n",
    "        self.vit_precision = config.vit_precision\n",
    "        print(f'Loading VIT. Use fp16: {config.vit_precision}')\n",
    "        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n",
    "            config.vit_model, config.img_size, config.drop_path_rate, \n",
    "            config.use_grad_checkpoint, config.vit_precision, config.vit_model_path,\n",
    "            temporal_downsample=config.temporal_downsample,\n",
    "            no_lmhra=config.no_lmhra, \n",
    "            double_lmhra=config.double_lmhra,\n",
    "            lmhra_reduction=config.lmhra_reduction, \n",
    "            gmhra_layers=config.gmhra_layers, \n",
    "            gmhra_drop_path_rate=config.gmhra_drop_path_rate,\n",
    "            gmhra_dropout=config.gmhra_dropout, \n",
    "        )\n",
    "        if config.freeze_vit:\n",
    "            print(\"freeze vision encoder\")\n",
    "            if not config.freeze_mhra:\n",
    "                open_list = []\n",
    "                for name, param in self.visual_encoder.named_parameters():\n",
    "                    if 'mhra' not in name:\n",
    "                        param.requires_grad = False\n",
    "                    else:\n",
    "                        open_list.append(name)\n",
    "\n",
    "            else:\n",
    "                for name, param in self.visual_encoder.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.visual_encoder = self.visual_encoder.eval()\n",
    "                self.visual_encoder.train = disabled_train\n",
    "                for name, param in self.ln_vision.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "                self.ln_vision = self.ln_vision.eval()\n",
    "                self.ln_vision.train = disabled_train\n",
    "            \n",
    "        self.blocks = nn.ModuleList([\n",
    "                Trans_Block(dim = config.embedding_size, num_heads = config.embedding_size//88, mlp_ratio= 4.3637)\n",
    "                    for i in range(10)])\n",
    "        self.norm = nn.LayerNorm(config.embedding_size)\n",
    "        self.fc_norm = nn.LayerNorm(config.embedding_size)\n",
    "\n",
    "        self.head = nn.Linear(config.embedding_size, config.frames)\n",
    "        self.score_head = nn.Linear(config.embedding_size, 1)\n",
    "        self.max_txt_len = config.max_txt_len\n",
    "\n",
    "\n",
    "    def vit_to_cpu(self):\n",
    "        self.ln_vision.to(\"cpu\")\n",
    "        self.ln_vision.float()\n",
    "        self.visual_encoder.to(\"cpu\")\n",
    "        self.visual_encoder.float()\n",
    "\n",
    "    def forward_features(self, interval_1,interval_2):\n",
    "\n",
    "        with self.maybe_autocast():\n",
    "            T = interval_1.shape[1]\n",
    "            # use_image = True if T == 1 else False\n",
    "            interval_1 = interval_1.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval_2 = interval_2.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "            interval1_embeds = self.ln_vision(self.visual_encoder(interval_1))\n",
    "            interval2_embeds = self.ln_vision(self.visual_encoder(interval_2))\n",
    "        x = torch.concat((interval1_embeds, interval2_embeds), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "            \n",
    "    def forward(self, interval_1,interval_2, y = None):\n",
    "        \n",
    "        x = self.forward_features(interval_1,interval_2)\n",
    "        x = self.norm(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        if y!= None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(x, y)\n",
    "            return x, loss\n",
    "        else:\n",
    "            return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from box import Box\n",
    "\n",
    "with open('/home/huimingsun/Desktop/NGP/video_sgements/config/config.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = Box(config_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT. Use fp16: fp32\n",
      "Temporal downsample: False\n",
      "Load ViT model from: /home/huimingsun/Desktop/NGP/video_sgements/checkpoint/eva_vit_g.pth\n",
      "Inflate: patch_embed.proj.weight, torch.Size([1408, 3, 14, 14]) => torch.Size([1408, 3, 1, 14, 14])\n",
      "freeze vision encoder\n"
     ]
    }
   ],
   "source": [
    "model = VSeg(config.model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = torch.rand(1, 5, 3,  224, 224).cuda()\n",
    "output = model(video,video)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img(self, interval_1,interval_2):\n",
    "\n",
    "    with self.maybe_autocast():\n",
    "        T = interval_1.shape[1]\n",
    "        # use_image = True if T == 1 else False\n",
    "        interval_1 = interval_1.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "        interval_2 = interval_2.permute(0, 2, 1, 3, 4) # [B,T,C,H,W] -> [B,C,T,H,W]\n",
    "\n",
    "        interval1_embeds = self.ln_vision(self.visual_encoder(interval_1))\n",
    "        interval2_embeds = self.ln_vision(self.visual_encoder(interval_2))\n",
    "\n",
    "        print(interval1_embeds.shape, interval2_embeds.shape)\n",
    "        \n",
    "        \n",
    "VSeg.encode_img = encode_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2572, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(1408, 1)(torch.randn(1, 1286*2,1408)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blocks = nn.ModuleList([\n",
    "                Trans_Block(dim = 1408, num_heads = 1408//88, mlp_ratio= 4.3637)\n",
    "                    for i in range(10)]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1286*2,1408).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2572, 1408])\n"
     ]
    }
   ],
   "source": [
    "for block in blocks:\n",
    "    x = block(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "feature map size torch.Size([5, 257, 1408])\n",
      "torch.Size([1, 1286, 1408]) torch.Size([1, 1286, 1408])\n"
     ]
    }
   ],
   "source": [
    "video = torch.rand(1, 5, 3,  224, 224).cuda()\n",
    "model.encode_img(video,video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 500])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from x_transformers import TransformerWrapper, Decoder\n",
    "\n",
    "model = TransformerWrapper(\n",
    "    num_tokens = 500,\n",
    "    max_seq_len = 1024,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 512,\n",
    "        depth = 12,\n",
    "        heads = 8\n",
    "    )\n",
    ").cuda()\n",
    "\n",
    "x = torch.randint(0, 256, (1, 1024)).cuda()\n",
    "\n",
    "model(x).shape # (1, 1024, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from recurrent_memory_transformer_pytorch import RecurrentMemoryTransformer\n",
    "\n",
    "model = RecurrentMemoryTransformer(\n",
    "    num_tokens = 20000,               # number of tokens\n",
    "    num_memory_tokens = 128,          # number of memory tokens, this will determine the bottleneck for information being passed to the future\n",
    "    dim = 512,                        # model dimensions\n",
    "    depth = 6,                        # transformer depth\n",
    "    causal = True,                    # autoregressive or not\n",
    "    dim_head = 64,                    # dimension per head\n",
    "    heads = 8,                        # heads\n",
    "    seq_len = 2048,                   # sequence length of a segment\n",
    "    use_flash_attn = True             # whether to use flash attention\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 20000]) torch.Size([1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 256, (1, 2048)).cuda()\n",
    "# print(x.shape)\n",
    "# x = torch.randint(0, 256, ( 197, 768))\n",
    "# print(x.shape)\n",
    "logits1, mem1, _ = model(x)        # (1, 1024, 20000), (1, 128, 512), None\n",
    "print(logits1.shape, mem1.shape)\n",
    "logits2, mem2, _ = model(x, mem1)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "logits3, mem3, _ = model(x, mem2)  # (1, 1024, 20000), (1, 128, 512), None\n",
    "\n",
    "# and so on ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
